{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Emotioni Estimate Model - YOLOv5l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "import json\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "import albumentations as A\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "Image.MAX_IMAGE_PIXELS = 933120000\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Dataset\n",
    "- 측면, 밝기 조절 등 추가 어노테이션 적용한 데이터 구축 (Train 4)  \n",
    "- 라벨링 수고를 덜기 위해, 앵커 박스로 크롭한 이미지 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROOT dir\n",
    "EXTRA_ROOT = r\"E:\\한국인 감정인식을 위한 복합 영상\\Training\"\n",
    "\n",
    "# Load Json data\n",
    "json_dir = [j for j in os.listdir(os.path.join(EXTRA_ROOT, \"label_json\"))]\n",
    "json_df = []\n",
    "for json_data in json_dir:\n",
    "    json_data = os.path.join(EXTRA_ROOT, \"label_json\", json_data)\n",
    "    with open(json_data) as jFile:\n",
    "        json_df.append(pd.DataFrame(json.load(jFile)))\n",
    "json_df = pd.concat(json_df, axis=0).reset_index(drop=True)\n",
    "\n",
    "# img data\n",
    "img_df = pd.DataFrame({\"PATH\":[img for img in os.listdir(os.path.join(EXTRA_ROOT, \"images\"))]})\n",
    "\n",
    "# extra df\n",
    "extra_mapping_df = pd.merge(\n",
    "    left=img_df, right=json_df, how=\"inner\",\n",
    "    left_on=[\"PATH\"], right_on=[\"filename\"]\n",
    ")[[\"PATH\", \"gender\", \"age\", \"faceExp_uploader\", \"annot_A\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annotate 정보 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Annot(annot):\n",
    "    annot = dict(annot)\n",
    "    return dict(annot[\"boxes\"])\n",
    "def get_maxX(annot): return annot[\"maxX\"]\n",
    "def get_maxY(annot): return annot[\"maxY\"]\n",
    "def get_minX(annot): return annot[\"minX\"]\n",
    "def get_minY(annot): return annot[\"minY\"]\n",
    "\n",
    "extra_mapping_df[\"annot_A\"] = extra_mapping_df[\"annot_A\"].apply(get_Annot)\n",
    "extra_mapping_df[\"A_maxX\"] = extra_mapping_df[\"annot_A\"].apply(get_maxX)\n",
    "extra_mapping_df[\"A_maxY\"] = extra_mapping_df[\"annot_A\"].apply(get_maxY)\n",
    "extra_mapping_df[\"A_minX\"] = extra_mapping_df[\"annot_A\"].apply(get_minX)\n",
    "extra_mapping_df[\"A_minY\"] = extra_mapping_df[\"annot_A\"].apply(get_minY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalization(cols): return float(cols[0]) / float(cols[1])\n",
    "\n",
    "# 원본 가로\n",
    "extra_mapping_df[\"O_W\"] = extra_mapping_df[\"SIZE\"].apply(lambda s: str(s).strip(\"(|)\").split(\",\")[0])\n",
    "# 원본 가로\n",
    "extra_mapping_df[\"O_H\"] = extra_mapping_df[\"SIZE\"].apply(lambda s: str(s).strip(\"(|)\").split(\",\")[1])\n",
    "extra_mapping_df[\"nor_A_maxX\"] = extra_mapping_df[[\"A_maxX\", \"O_W\"]].apply(normalization, axis=1)\n",
    "extra_mapping_df[\"nor_A_maxY\"] = extra_mapping_df[[\"A_maxY\", \"O_H\"]].apply(normalization, axis=1)\n",
    "extra_mapping_df[\"nor_A_minX\"] = extra_mapping_df[[\"A_minX\", \"O_W\"]].apply(normalization, axis=1)\n",
    "extra_mapping_df[\"nor_A_minY\"] = extra_mapping_df[[\"A_minY\", \"O_H\"]].apply(normalization, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate X, Y, W, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_XorY(cols): return (cols[0]+cols[1])/2.0\n",
    "def get_WorH(cols): return cols[1] - cols[0]\n",
    "\n",
    "extra_mapping_df[\"AX\"] = extra_mapping_df[[\"nor_A_minX\", \"nor_A_maxX\"]].apply(get_XorY, axis=1)\n",
    "extra_mapping_df[\"AY\"] = extra_mapping_df[[\"nor_A_minY\", \"nor_A_maxY\"]].apply(get_XorY, axis=1)\n",
    "extra_mapping_df[\"AW\"] = extra_mapping_df[[\"nor_A_minX\", \"nor_A_maxX\"]].apply(get_WorH, axis=1)\n",
    "extra_mapping_df[\"AH\"] = extra_mapping_df[[\"nor_A_minY\", \"nor_A_maxY\"]].apply(get_WorH, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crop 및 사이즈 조절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 20906/83072 [59:32<2:58:28,  5.81it/s] c:\\Users\\spec3\\anaconda3\\envs\\NN\\lib\\site-packages\\PIL\\TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      " 26%|██▌       | 21196/83072 [1:00:22<3:02:54,  5.64it/s]c:\\Users\\spec3\\anaconda3\\envs\\NN\\lib\\site-packages\\PIL\\TiffImagePlugin.py:845: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 83072/83072 [3:36:48<00:00,  6.39it/s]   \n"
     ]
    }
   ],
   "source": [
    "# 원본 이미지 사이즈 가져오기 및 크롭, 사이즈 조절\n",
    "def get_size_and_crop_640x640(cols):\n",
    "    path, maxX, maxY, minX, minY = cols\n",
    "    save_path = os.path.join(EXTRA_ROOT, \"Crop640\", path)\n",
    "    path = os.path.join(EXTRA_ROOT, \"images\", path)\n",
    "    try:\n",
    "        img = Image.open(path)\n",
    "        osize = img.size # 원본 이미지 사이즈\n",
    "        crop_img = img.crop((minX, minY, maxX, maxY))\n",
    "        crop_img = crop_img.resize((640, 640))\n",
    "        crop_img.save(save_path) # Crop 이미지 저장\n",
    "        return osize\n",
    "    except: print(path)\n",
    "    \n",
    "extra_mapping_df[\"SIZE\"] = extra_mapping_df[[\"PATH\", \"A_maxX\", \"A_maxY\", \"A_minX\", \"A_minY\"]].progress_apply(get_size_and_crop_640x640, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 스플릿 및 증강"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 8856\n",
    "VAL_SIZE = 2214\n",
    "\n",
    "# 이상치 제거\n",
    "cols = [\"nor_A_minX\", \"nor_A_minY\", \"nor_A_maxX\", \"nor_A_maxY\"]\n",
    "for col in cols:\n",
    "    extra_mapping_df = extra_mapping_df[extra_mapping_df[col]<1]\n",
    "    extra_mapping_df = extra_mapping_df[extra_mapping_df[col]>0]\n",
    "\n",
    "label_dict = {\"기쁨\":0, \"당황\":1, \"불안\":1, \"분노\":2, \"상처\":3, \"슬픔\":3, \"중립\":4}\n",
    "extra_mapping_df[\"label\"] = extra_mapping_df[\"faceExp_uploader\"].apply(lambda x: label_dict[x])\n",
    "\n",
    "train_df = []\n",
    "val_df = []\n",
    "for label in label_dict.keys():\n",
    "    label_df = extra_mapping_df[extra_mapping_df[\"faceExp_uploader\"]==label]\n",
    "    if label in [\"당황\", \"불안\", \"상처\", \"슬픔\"]:\n",
    "        train_df.append(extra_mapping_df.sample(n=int(TRAIN_SIZE/2), replace=False))\n",
    "        temp = pd.concat(train_df, axis=0)\n",
    "        exclude = extra_mapping_df.loc[list(set(extra_mapping_df.index) - set(temp.index))].copy() # 중복 방지\n",
    "        val_df.append(exclude.sample(n=int(VAL_SIZE/2), replace=False))\n",
    "    else:\n",
    "        train_df.append(extra_mapping_df.sample(n=TRAIN_SIZE, replace=False))\n",
    "        temp = pd.concat(train_df, axis=0)\n",
    "        exclude = extra_mapping_df.loc[list(set(extra_mapping_df.index) - set(temp.index))].copy() # 중복 방지\n",
    "        val_df.append(exclude.sample(n=VAL_SIZE, replace=False))\n",
    "        \n",
    "train_df = pd.concat(train_df, axis=0)\n",
    "val_df = pd.concat(val_df, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 14097/44280 [1:00:43<1:42:12,  4.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed8e05a7770502606d8278a279c624e4fb7e681fd09766c71b4c8eb8d3af7a5b_여_30_슬픔_실외 자연환경_20201206211011-007-010.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 32110/44280 [2:22:14<39:11,  5.18it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f9df7685b93fb5ce2c02f1c995be558095a32aa5946837ba75279330eece3027_여_20_당황_문화재 및 유적지_20201208120404-009-015.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44280/44280 [3:18:37<00:00,  3.72it/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72996    None\n",
       "38368    None\n",
       "75426    None\n",
       "7355     None\n",
       "73159    None\n",
       "         ... \n",
       "42931    None\n",
       "30494    None\n",
       "81276    None\n",
       "54918    None\n",
       "31675    None\n",
       "Length: 44280, dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DataAug(cols):\n",
    "    path, x, y, w, h, label = cols.values\n",
    "    txt = path.split(\".\")[0]\n",
    "    try:\n",
    "        # 데이터 증강 정의\n",
    "        bbox = [[x, y, w, h, label]]\n",
    "        transform = A.Compose([\n",
    "            A.Resize(height=640, width=640),\n",
    "            A.Affine(shear=[-45, 45]),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=(-0.4, 0), contrast_limit=0, p=0.6),\n",
    "                A.Blur(p=0.6, blur_limit=(5, 30))\n",
    "            ])\n",
    "        ], bbox_params=A.BboxParams(format=\"yolo\"))\n",
    "        # 증강 적용\n",
    "        img = Image.open(os.path.join(EXTRA_ROOT, \"images\", path))\n",
    "        aug = transform(image=np.array(img), bboxes=bbox)\n",
    "        # 이미지 저장\n",
    "        (Image.fromarray(aug[\"image\"])).save(os.path.join(EXTRA_ROOT, \"aug\", \"Train\", path))\n",
    "        # 라벨 저장\n",
    "        tx, ty, tw, th, label = aug[\"bboxes\"][0]\n",
    "        with open(os.path.join(EXTRA_ROOT, \"aug\", \"Train\", txt+\".txt\"), \"w\") as txt:\n",
    "            txt.write(f\"{label} {tx} {ty} {tw} {th}\")\n",
    "    except: print(path)\n",
    "    \n",
    "train_df[[\"PATH\", \"AX\", \"AY\", \"AW\", \"AH\", \"label\"]].progress_apply(DataAug, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4696/11070 [19:21<20:18,  5.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e884e3bee913250e56fee74841c8f9aacdbea2c5e70df40352582e7300e39947_남_20_중립_도심 환경_20201207020727-008-002.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 10392/11070 [42:11<02:21,  4.80it/s] c:\\Users\\spec3\\anaconda3\\envs\\NN\\lib\\site-packages\\PIL\\TiffImagePlugin.py:845: UserWarning: Corrupt EXIF data.  Expecting to read 2 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "100%|██████████| 11070/11070 [44:46<00:00,  4.12it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "38755    None\n",
       "70676    None\n",
       "79776    None\n",
       "13711    None\n",
       "36776    None\n",
       "         ... \n",
       "24969    None\n",
       "42351    None\n",
       "23001    None\n",
       "45282    None\n",
       "72233    None\n",
       "Length: 11070, dtype: object"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def DataAug(cols):\n",
    "    path, x, y, w, h, label = cols.values\n",
    "    txt = path.split(\".\")[0]\n",
    "    try:\n",
    "        # 데이터 증강 정의\n",
    "        bbox = [[x, y, w, h, label]]\n",
    "        transform = A.Compose([\n",
    "            A.Resize(height=640, width=640),\n",
    "            A.Affine(shear=[-45, 45]),\n",
    "            A.OneOf([\n",
    "                A.RandomBrightnessContrast(brightness_limit=(-0.4, 0), contrast_limit=0, p=0.6),\n",
    "                A.Blur(p=0.6, blur_limit=(5, 30))\n",
    "            ])\n",
    "        ], bbox_params=A.BboxParams(format=\"yolo\"))\n",
    "        # 증강 적용\n",
    "        img = Image.open(os.path.join(EXTRA_ROOT, \"images\", path))\n",
    "        aug = transform(image=np.array(img), bboxes=bbox)\n",
    "        # 이미지 저장\n",
    "        (Image.fromarray(aug[\"image\"])).save(os.path.join(EXTRA_ROOT, \"aug\", \"Val\", path))\n",
    "        # 라벨 저장\n",
    "        tx, ty, tw, th, label = aug[\"bboxes\"][0]\n",
    "        with open(os.path.join(EXTRA_ROOT, \"aug\", \"Val\", txt+\".txt\"), \"w\") as txt:\n",
    "            txt.write(f\"{label} {tx} {ty} {tw} {th}\")\n",
    "    except: print(path)\n",
    "    \n",
    "val_df[[\"PATH\", \"AX\", \"AY\", \"AW\", \"AH\", \"label\"]].progress_apply(DataAug, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "et = pd.DataFrame({\"PATH\":os.listdir(os.path.join(EXTRA_ROOT, \"aug\", \"Train\"))})\n",
    "ev = pd.DataFrame({\"PATH\":os.listdir(os.path.join(EXTRA_ROOT, \"aug\", \"Val\"))})\n",
    "\n",
    "et[\"Label\"] = et[\"PATH\"].apply(lambda x: x.split(\"_\")[3]) \n",
    "ev[\"Label\"] = ev[\"PATH\"].apply(lambda x: x.split(\"_\")[3])\n",
    "\n",
    "et[\"TXT\"] = et[\"PATH\"].apply(lambda x: x.split(\".\")[0]+\".txt\") \n",
    "ev[\"TXT\"] = ev[\"PATH\"].apply(lambda x: x.split(\".\")[0]+\".txt\")\n",
    "\n",
    "et[\"R\"] = et[\"PATH\"].apply(lambda x: x.split(\".\")[0]) \n",
    "ev[\"R\"] = ev[\"PATH\"].apply(lambda x: x.split(\".\")[0])\n",
    "\n",
    "et = et.drop_duplicates(\"R\")\n",
    "ev = ev.drop_duplicates(\"R\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mov_vir_t(cols):\n",
    "    try:\n",
    "        path, txt = cols[0], cols[1]\n",
    "        shutil.copy(os.path.join(EXTRA_ROOT, \"aug\", \"Train\", path), os.path.join(T7, \"Train\", path))\n",
    "        shutil.copy(os.path.join(EXTRA_ROOT, \"aug\", \"Train\", txt), os.path.join(T7, \"Train\", txt))\n",
    "    except: print(path)\n",
    "    \n",
    "def mov_vir_v(cols):\n",
    "    path, txt = cols[0], cols[1]\n",
    "    try:\n",
    "        shutil.copy(os.path.join(EXTRA_ROOT, \"aug\", \"Val\", path), os.path.join(T7, \"Val\", path))\n",
    "        shutil.copy(os.path.join(EXTRA_ROOT, \"aug\", \"Val\", txt), os.path.join(T7, \"Val\", txt))\n",
    "    except: print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "상처    5240\n",
       "슬픔    5186\n",
       "분노    5148\n",
       "기쁨    5078\n",
       "불안    5033\n",
       "당황    4970\n",
       "중립    4763\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et[\"Label\"].value_counts() # 4760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2563/2563 [08:31<00:00,  5.01it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54344    None\n",
       "68543    None\n",
       "6054     None\n",
       "36552    None\n",
       "24326    None\n",
       "         ... \n",
       "56998    None\n",
       "32976    None\n",
       "50796    None\n",
       "28864    None\n",
       "34310    None\n",
       "Length: 2563, dtype: object"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "et[et[\"Label\"]==\"슬픔\"].sample(2563)[[\"PATH\", \"TXT\"]].progress_apply(mov_vir_t, axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real Time Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\spec3/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2023-4-1 Python-3.9.15 torch-1.12.1 CUDA:0 (NVIDIA GeForce RTX 4080, 16376MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 267 layers, 46129818 parameters, 0 gradients, 107.7 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=\"./last.pt\", force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "[]\n",
      "['Neutral']\n",
      "[]\n",
      "[]\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n",
      "['Neutral']\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0) # webcam or connected cam\n",
    "PATH = r\"C:\\Users\\spec3\\Downloads\\켈리클락슨을 당황시킨 방탄 미국 팬들 함성.mp4\"\n",
    "# PATH = r\"C:\\Users\\spec3\\Downloads\\[인생은 아름다워] '찐'관객 추천 영상.mp4\"\n",
    "# cap = cv2.VideoCapture(PATH) # Loaded video\n",
    "\n",
    "thr = 0.5 # 검출 정확도\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    results = model(frame)\n",
    "    cv2.imshow(\"Detection\", np.squeeze(results.render()))\n",
    "    # 라벨 출력\n",
    "    labels = results.pandas().xyxy[0]\n",
    "    detect_labels = labels[labels[\"confidence\"]>thr][\"name\"]\n",
    "    print(detect_labels.tolist())\n",
    "    \n",
    "    if cv2.waitKey(10)&0xFF==ord(\"q\"): break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6170c7e636c2dbd0f88cb8f557866518ac6c7d83ee4f2709c7df3161954bfcc9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
